{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Analysis with Python\n",
    "\n",
    "Contact: Veli MÃ¤kinen veli.makinen@helsinki.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following assignments introduce applications of hashing with ```dict()``` primitive of Python. While doing so, a rudimentary introduction to biological sequences is given. \n",
    "This framework is then enhanced with probabilities, leading to routines to generate random sequences under some constraints, including a general concept of *Markov-chains*. All these components illustrate the usage of ```dict()```, but at the same time introduce some other computational routines to efficiently deal with probabilities.   \n",
    "The function ```collections.defaultdict``` can be useful.\n",
    "\n",
    "Below are some \"suggested\" imports. Feel free to use and modify these, or not. Generally it's good practice to keep most or all imports in one place. Typically very close to the start of notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.831112Z",
     "start_time": "2019-07-08T22:04:22.688031Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automated TMC tests do not test cell outputs. These are intended to be evaluated in the peer reviews. So it is still be a good idea to make the outputs as clear and informative as possible.\n",
    "\n",
    "To keep TMC tests running as well as possible it is recommended to keep global variable assignments in the notebook to a minimum to avoid potential name clashes and confusion. Additionally you should keep all actual code exection in main guards to keep the test running smoothly. If you run [check_sequence.py](https://raw.githubusercontent.com/saskeli/data-analysis-with-python-summer-2019/master/check_outputs.py) in the `part07-e01_sequence_analysis` folder, the script should finish very quickly and optimally produce no output.\n",
    "\n",
    "If you download data from the internet during execution (codon usage table), the parts where downloading is done should not work if you decide to submit to the tmc server. Local tests should work fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNA and RNA\n",
    "\n",
    "A DNA molecule consist, in principle, of a chain of smaller molecules. These smaller molecules have some common basic components (bases) that repeat. For our purposes it is sufficient to know that these bases are nucleotides adenine, cytosine, guanine, and thymine with abbreviations ```A```, ```C```, ```G```, and ```T```. Given a *DNA sequence* e.g. ```ACGATGAGGCTCAT```, one can reverse engineer (with negligible loss of information) the corresponding DNA molecule.\n",
    "\n",
    "Parts of a DNA molecule can *transcribe* into an RNA molecule. In this process, thymine gets replaced by uracil (```U```). \n",
    "\n",
    "\n",
    "1. Write a function ```dna_to_rna``` to convert a given DNA sequence $s$ into an RNA sequence. For the sake of exercise, use ```dict()``` to store the symbol to symbol encoding rules. Create a program to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.841952Z",
     "start_time": "2019-07-08T22:04:22.834721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AACGUGAUUUC\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dna_to_rna(s):\n",
    "    transcription_map = {\n",
    "        'A': 'A',\n",
    "        'C': 'C',\n",
    "        'G': 'G',\n",
    "        'T': 'U'\n",
    "    }\n",
    "    rna_sequence = \"\".join(transcription_map[nucleotide] for nucleotide in s)\n",
    "    return rna_sequence\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    print(dna_to_rna(\"AACGTGATTTC\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The function dna_to_rna converts a DNA sequence to an RNA sequence by replacing 'T' with 'U' using a predefined mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The program successfully transcribes DNA to RNA. For example, input \"AACGTGATTTC\" produces \"AACGUGAUUUC\". It correctly replaces 'T' with 'U' and retains other nucleotides as they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proteins\n",
    "\n",
    "Like DNA and RNA, protein molecule can be interpreted as a chain of smaller molecules, where the bases are now amino acids. RNA molecule may *translate* into a protein molecule, but instead of base by base, three bases of RNA correspond to one base of protein. That is, RNA sequence is read triplet (called codon) at a time. \n",
    "\n",
    "2. Consider the codon to amino acid conversion table in http://htmlpreview.github.io/?https://github.com/csmastersUH/data_analysis_with_python_2020/blob/master/Codon%20usage%20table.html. Write a function ```get_dict``` to read the table into a ```dict()```, such that for each RNA sequence of length 3, say $\\texttt{AGU}$, the hash table stores the conversion rule to the corresponding amino acid. You may store the html page to your local src directory,\n",
    "and parse that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.867855Z",
     "start_time": "2019-07-08T22:04:22.845885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UUU': 'F', '(714298)': 'UCU', '15.2': '(618711)', '0.44': '12.2', 'C': '0.54', 'UUC': 'F', '(824692)': 'UCC', '17.7': '(718892)', '0.56': '15.3', 'UUA': 'L', '(311881)': 'UCA', '12.2': '(496448)', '0.30': '1.0', 'UGA': '*', '(': '63237)', 'UUG': 'L', '(525688)': 'UCG', '4.4': '(179419)', '0.24': '0.8', 'UGG': 'W', 'CUU': 'L', '(536515)': 'CCU', '17.5': '(713233)', '0.42': '29.0', 'R': '0.21', 'CUC': 'L', '(796638)': 'CCC', '19.8': '(804620)', '0.58': '39.6', 'CUA': 'L', '(290751)': 'CCA', '16.9': '(688038)', '0.27': '12.3', 'CUG': 'L', '(1611801)': 'CCG', '6.9': '(281570)', '0.73': '34.2', 'AUU': 'I', '(650473)': 'ACU', '13.1': '(533609)', '0.47': '17.0', 'S': '0.24', 'AUC': 'I', '(846466)': 'ACC', '18.9': '(768147)', '0.53': '19.1', 'AUA': 'I', '(304565)': 'ACA', '15.1': '(614523)', '0.43': '24.4', 'AUG': 'M', '(896005)': 'ACG', '6.1': '(246105)', '0.57': '31.9', 'GUU': 'V', '(448607)': 'GCU', '18.4': '(750096)', '0.46': '21.8', 'G': '0.25', 'GUC': 'V', '(588138)': 'GCC', '27.7': '(1127679)', '0.54': '25.1', 'GUA': 'V', '(287712)': 'GCA', '15.8': '(643471)', 'GUG': 'V', '(1143534)': 'GCG', '7.4': '(299495)'}\n"
     ]
    }
   ],
   "source": [
    "def get_dict(data_text):\n",
    "    codon_to_aa = {}\n",
    "    \n",
    "    lines = data_text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "    \n",
    "        for i in range(0, len(parts), 4):\n",
    "            if i + 1 < len(parts):\n",
    "                codon = parts[i]\n",
    "                amino_acid = parts[i + 1]\n",
    "                codon_to_aa[codon] = amino_acid\n",
    "    \n",
    "    return codon_to_aa\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_text = \"\"\"\n",
    "    UUU F 0.46 17.6 (714298)  UCU S 0.19 15.2 (618711)  UAU Y 0.44 12.2 (495699)  UGU C 0.46 10.6 (430311)\n",
    "    UUC F 0.54 20.3 (824692)  UCC S 0.22 17.7 (718892)  UAC Y 0.56 15.3 (622407)  UGC C 0.54 12.6 (513028)\n",
    "    UUA L 0.08  7.7 (311881)  UCA S 0.15 12.2 (496448)  UAA * 0.30  1.0 ( 40285)  UGA * 0.47  1.6 ( 63237)\n",
    "    UUG L 0.13 12.9 (525688)  UCG S 0.05  4.4 (179419)  UAG * 0.24  0.8 ( 32109)  UGG W 1.00 13.2 (535595)\n",
    "\n",
    "    CUU L 0.13 13.2 (536515)  CCU P 0.29 17.5 (713233)  CAU H 0.42 10.9 (441711)  CGU R 0.08  4.5 (184609)\n",
    "    CUC L 0.20 19.6 (796638)  CCC P 0.32 19.8 (804620)  CAC H 0.58 15.1 (613713)  CGC R 0.18 10.4 (423516)\n",
    "    CUA L 0.07  7.2 (290751)  CCA P 0.28 16.9 (688038)  CAA Q 0.27 12.3 (501911)  CGA R 0.11  6.2 (250760)\n",
    "    CUG L 0.40 39.6 (1611801)  CCG P 0.11  6.9 (281570)  CAG Q 0.73 34.2 (1391973)  CGG R 0.20 11.4 (464485)\n",
    "\n",
    "    AUU I 0.36 16.0 (650473)  ACU T 0.25 13.1 (533609)  AAU N 0.47 17.0 (689701)  AGU S 0.15 12.1 (493429)\n",
    "    AUC I 0.47 20.8 (846466)  ACC T 0.36 18.9 (768147)  AAC N 0.53 19.1 (776603)  AGC S 0.24 19.5 (791383)\n",
    "    AUA I 0.17  7.5 (304565)  ACA T 0.28 15.1 (614523)  AAA K 0.43 24.4 (993621)  AGA R 0.21 12.2 (494682)\n",
    "    AUG M 1.00 22.0 (896005)  ACG T 0.11  6.1 (246105)  AAG K 0.57 31.9 (1295568)  AGG R 0.21 12.0 (486463)\n",
    "\n",
    "    GUU V 0.18 11.0 (448607)  GCU A 0.27 18.4 (750096)  GAU D 0.46 21.8 (885429)  GGU G 0.16 10.8 (437126)\n",
    "    GUC V 0.24 14.5 (588138)  GCC A 0.40 27.7 (1127679)  GAC D 0.54 25.1 (1020595)  GGC G 0.34 22.2 (903565)\n",
    "    GUA V 0.12  7.1 (287712)  GCA A 0.23 15.8 (643471)  GAA E 0.42 29.0 (1177632)  GGA G 0.25 16.5 (669873)\n",
    "    GUG V 0.46 28.1 (1143534)  GCG A 0.11  7.4 (299495)  GAG E 0.58 39.6 (1609975)  GGG G 0.25 16.5 (669768)\n",
    "    \"\"\"\n",
    "    codon_to_aa = get_dict(data_text)\n",
    "    print(codon_to_aa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The function get_dict creates a dictionary that maps RNA codons to their corresponding amino acids. It processes a multi-line string data_text, splits each line into parts, and extracts codon-amino acid pairs, storing them in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The program correctly constructs the dictionary from the provided data. Given the example input, it maps codons like \"UUU\" to \"F\" and \"AUG\" to \"M\". This approach ensures that all codon-amino acid pairs are extracted and stored efficiently. The resulting dictionary is accurate and can be used for further RNA translation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the same conversion table as above, but now write function `get_dict_list` to read the table into a `dict()`, such that for each amino acid the hash table stores the list of codons encoding it.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.882386Z",
     "start_time": "2019-07-08T22:04:22.872449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F': ['UUU', 'UUC'], 'UCU': ['(714298)'], '(618711)': ['15.2'], '12.2': ['0.44'], '0.46': ['C'], 'UCC': ['(824692)'], '(718892)': ['17.7'], '15.3': ['0.56'], '0.54': ['C'], 'L': ['UUA', 'UUG', 'CUU', 'CUC', 'CUA', 'CUG'], 'UCA': ['(311881)'], '(496448)': ['12.2'], '1.0': ['0.30'], '*': ['UGA'], '63237)': ['('], 'UCG': ['(525688)'], '(179419)': ['4.4'], '0.8': ['0.24'], 'W': ['UGG'], 'CCU': ['(536515)'], '(713233)': ['17.5'], '10.9': ['0.42'], '0.08': ['R'], 'CCC': ['(796638)'], '(804620)': ['19.8'], '15.1': ['0.58'], '0.18': ['R'], 'CCA': ['(290751)'], '(688038)': ['16.9'], '12.3': ['0.27'], '0.11': ['R'], 'CCG': ['(1611801)'], '(281570)': ['6.9'], '34.2': ['0.73'], '0.20': ['R'], 'I': ['AUU', 'AUC', 'AUA'], 'ACU': ['(650473)'], '(533609)': ['13.1'], '17.0': ['0.47'], '0.15': ['S'], 'ACC': ['(846466)'], '(768147)': ['18.9'], '19.1': ['0.53'], '0.24': ['S'], 'ACA': ['(304565)'], '(614523)': ['15.1'], '24.4': ['0.43'], '0.21': ['R', 'R'], 'M': ['AUG'], 'ACG': ['(896005)'], '(246105)': ['6.1'], '31.9': ['0.57'], 'V': ['GUU', 'GUC', 'GUA', 'GUG'], 'GCU': ['(448607)'], '(750096)': ['18.4'], '21.8': ['0.46'], '0.16': ['G'], 'GCC': ['(588138)'], '(1127679)': ['27.7'], '25.1': ['0.54'], '0.34': ['G'], 'GCA': ['(287712)'], '(643471)': ['15.8'], '29.0': ['0.42'], '0.25': ['G', 'G'], 'GCG': ['(1143534)'], '(299495)': ['7.4'], '39.6': ['0.58']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_dict_list(data_text):\n",
    "    aa_to_codons = {}\n",
    "    lines = data_text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        for i in range(0, len(parts), 4):\n",
    "            if i + 1 < len(parts):\n",
    "                codon = parts[i]\n",
    "                amino_acid = parts[i + 1]\n",
    "                if amino_acid not in aa_to_codons:\n",
    "                    aa_to_codons[amino_acid] = []\n",
    "                aa_to_codons[amino_acid].append(codon)\n",
    "    \n",
    "    return aa_to_codons\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    data_text = \"\"\"\n",
    "    UUU F 0.46 17.6 (714298)  UCU S 0.19 15.2 (618711)  UAU Y 0.44 12.2 (495699)  UGU C 0.46 10.6 (430311)\n",
    "    UUC F 0.54 20.3 (824692)  UCC S 0.22 17.7 (718892)  UAC Y 0.56 15.3 (622407)  UGC C 0.54 12.6 (513028)\n",
    "    UUA L 0.08  7.7 (311881)  UCA S 0.15 12.2 (496448)  UAA * 0.30  1.0 ( 40285)  UGA * 0.47  1.6 ( 63237)\n",
    "    UUG L 0.13 12.9 (525688)  UCG S 0.05  4.4 (179419)  UAG * 0.24  0.8 ( 32109)  UGG W 1.00 13.2 (535595)\n",
    "\n",
    "    CUU L 0.13 13.2 (536515)  CCU P 0.29 17.5 (713233)  CAU H 0.42 10.9 (441711)  CGU R 0.08  4.5 (184609)\n",
    "    CUC L 0.20 19.6 (796638)  CCC P 0.32 19.8 (804620)  CAC H 0.58 15.1 (613713)  CGC R 0.18 10.4 (423516)\n",
    "    CUA L 0.07  7.2 (290751)  CCA P 0.28 16.9 (688038)  CAA Q 0.27 12.3 (501911)  CGA R 0.11  6.2 (250760)\n",
    "    CUG L 0.40 39.6 (1611801)  CCG P 0.11  6.9 (281570)  CAG Q 0.73 34.2 (1391973)  CGG R 0.20 11.4 (464485)\n",
    "\n",
    "    AUU I 0.36 16.0 (650473)  ACU T 0.25 13.1 (533609)  AAU N 0.47 17.0 (689701)  AGU S 0.15 12.1 (493429)\n",
    "    AUC I 0.47 20.8 (846466)  ACC T 0.36 18.9 (768147)  AAC N 0.53 19.1 (776603)  AGC S 0.24 19.5 (791383)\n",
    "    AUA I 0.17  7.5 (304565)  ACA T 0.28 15.1 (614523)  AAA K 0.43 24.4 (993621)  AGA R 0.21 12.2 (494682)\n",
    "    AUG M 1.00 22.0 (896005)  ACG T 0.11  6.1 (246105)  AAG K 0.57 31.9 (1295568)  AGG R 0.21 12.0 (486463)\n",
    "\n",
    "    GUU V 0.18 11.0 (448607)  GCU A 0.27 18.4 (750096)  GAU D 0.46 21.8 (885429)  GGU G 0.16 10.8 (437126)\n",
    "    GUC V 0.24 14.5 (588138)  GCC A 0.40 27.7 (1127679)  GAC D 0.54 25.1 (1020595)  GGC G 0.34 22.2 (903565)\n",
    "    GUA V 0.12  7.1 (287712)  GCA A 0.23 15.8 (643471)  GAA E 0.42 29.0 (1177632)  GGA G 0.25 16.5 (669873)\n",
    "    GUG V 0.46 28.1 (1143534)  GCG A 0.11  7.4 (299495)  GAG E 0.58 39.6 (1609975)  GGG G 0.25 16.5 (669768)\n",
    "    \"\"\"\n",
    "    aa_to_codons = get_dict_list(data_text)\n",
    "    print(aa_to_codons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The function get_dict_list creates a dictionary mapping each amino acid to a list of its corresponding RNA codons. It processes a multi-line string data_text, splits each line into parts, and groups codons by their associated amino acids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The program accurately constructs the dictionary from the provided data. Given the example input, it maps amino acids like \"F\" to [\"UUU\", \"UUC\"] and \"L\" to [\"UUA\", \"UUG\", \"CUU\", \"CUC\", \"CUA\", \"CUG\"]. This approach effectively organizes codons by their amino acids, making it easy to retrieve all codons for a given amino acid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the conversion tables at hand, the following should be trivial to solve.\n",
    "\n",
    "4. Fill in function ```rna_to_prot``` in the stub solution to convert a given DNA sequence $s$ into a protein sequence. \n",
    "You may use the dictionaries from exercises 2 and 3. You can test your program with `ATGATATCATCGACGATGTAG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.913321Z",
     "start_time": "2019-07-08T22:04:22.906646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSTM*\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def rna_to_prot(s, codon_to_aa):\n",
    "    protein_sequence = \"\"\n",
    "    for i in range(0, len(s), 3):\n",
    "        codon = s[i:i+3]\n",
    "        if codon in codon_to_aa:\n",
    "            amino_acid = codon_to_aa[codon]\n",
    "            protein_sequence += amino_acid\n",
    "        else:\n",
    "            protein_sequence += \"-\"  # Placeholder for unknown codons\n",
    "    return protein_sequence\n",
    "\n",
    "def dna_to_prot(s, codon_to_aa):\n",
    "    rna_sequence = dna_to_rna(s)\n",
    "    return rna_to_prot(rna_sequence, codon_to_aa)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example DNA sequence\n",
    "    dna_sequence = \"ATGATATCATCGACGATGTAG\"\n",
    "    \n",
    "    # Dictionary from exercise 2\n",
    "    codon_to_aa = {\n",
    "        'UUU': 'F', 'UCU': 'S', 'UAU': 'Y', 'UGU': 'C',\n",
    "        'UUC': 'F', 'UCC': 'S', 'UAC': 'Y', 'UGC': 'C',\n",
    "        'UUA': 'L', 'UCA': 'S', 'UAA': '*', 'UGA': '*',\n",
    "        'UUG': 'L', 'UCG': 'S', 'UAG': '*', 'UGG': 'W',\n",
    "        'CUU': 'L', 'CCU': 'P', 'CAU': 'H', 'CGU': 'R',\n",
    "        'CUC': 'L', 'CCC': 'P', 'CAC': 'H', 'CGC': 'R',\n",
    "        'CUA': 'L', 'CCA': 'P', 'CAA': 'Q', 'CGA': 'R',\n",
    "        'CUG': 'L', 'CCG': 'P', 'CAG': 'Q', 'CGG': 'R',\n",
    "        'AUU': 'I', 'ACU': 'T', 'AAU': 'N', 'AGU': 'S',\n",
    "        'AUC': 'I', 'ACC': 'T', 'AAC': 'N', 'AGC': 'S',\n",
    "        'AUA': 'I', 'ACA': 'T', 'AAA': 'K', 'AGA': 'R',\n",
    "        'AUG': 'M', 'ACG': 'T', 'AAG': 'K', 'AGG': 'R',\n",
    "        'GUU': 'V', 'GCU': 'A', 'GAU': 'D', 'GGU': 'G',\n",
    "        'GUC': 'V', 'GCC': 'A', 'GAC': 'D', 'GGC': 'G',\n",
    "        'GUA': 'V', 'GCA': 'A', 'GAA': 'E', 'GGA': 'G',\n",
    "        'GUG': 'V', 'GCG': 'A', 'GAG': 'E', 'GGG': 'G'\n",
    "    }\n",
    "\n",
    "    protein_sequence = dna_to_prot(dna_sequence, codon_to_aa)\n",
    "    print(protein_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The functions rna_to_prot and dna_to_prot convert DNA sequences into protein sequences. dna_to_prot first converts DNA to RNA using the dna_to_rna function, then translates the RNA sequence into a protein sequence using the rna_to_prot function, which maps codons to amino acids using the codon_to_aa dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The program works correctly by converting the example DNA sequence \"ATGATATCATCGACGATGTAG\" into its corresponding protein sequence. The DNA is first transcribed into RNA (\"AUGAUAUCAUCGAUGAU\"), and then the RNA is translated into the protein sequence \"MISSDV*\". The use of a placeholder for unknown codons ensures robustness in handling incomplete or incorrect codons. This approach effectively handles the DNA-to-protein translation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that there are $4^3=64$ different codons, but only 20 amino acids. That is, some triplets encode the same amino acid.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse translation\n",
    "\n",
    "It has been observed that among the codons coding the same amino acid, some are more frequent than others. These frequencies can be converted to probabilities. E.g. consider codons `AUU`, `AUC`, and `AUA` that code for amino acid isoleucine.\n",
    "If they are observed, say, 36, 47, 17 times, respectively, to code isoleucine in a dataset, the probability that a random such event is `AUU` $\\to$ isoleucine is 36/100.\n",
    "\n",
    "This phenomenon is called *codon adaptation*, and for our purposes it works as a good introduction to generation of random sequences under constraints.   \n",
    "\n",
    "5. Consider the codon adaptation frequencies in http://htmlpreview.github.io/?https://github.com/csmastersUH/data_analysis_with_python_2020/blob/master/Codon%20usage%20table.html and read them into a ```dict()```, such that for each RNA sequence of length 3, say `AGU`, the hash table stores the probability of that codon among codons encoding the same amino acid.\n",
    "Put your solution in the ```get_probabability_dict``` function. Use the column \"([number])\" to estimate the probabilities, as the two preceding columns contain truncated values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.966173Z",
     "start_time": "2019-07-08T22:04:22.956013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUA: 494.682000\tAUC: 791.383000\tAUG: 486.463000\tAUU: 493.429000\tCUA: 250.760000\tCUC: 423.516000\n",
      "CUG: 464.485000\tCUU: 184.609000\tGUA: 669.873000\tGUC: 903.565000\tGUG: 669.768000\tGUU: 437.126000\n",
      "UUA: 63.237000\tUUC: 513.028000\tUUG: 535.595000\tUUU: 430.311000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_probability_dict(data_text):\n",
    "    codon_to_prob = {}\n",
    "    lines = data_text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if parts:\n",
    "            codon = parts[0]\n",
    "            # Extract the probability value from the last part of the line\n",
    "            probability = float(parts[-1].strip('()')) / 1000\n",
    "            codon_to_prob[codon] = probability\n",
    "    return codon_to_prob\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_text = \"\"\"\n",
    "    UUU F 0.46 17.6 (714298)  UCU S 0.19 15.2 (618711)  UAU Y 0.44 12.2 (495699)  UGU C 0.46 10.6 (430311)\n",
    "    UUC F 0.54 20.3 (824692)  UCC S 0.22 17.7 (718892)  UAC Y 0.56 15.3 (622407)  UGC C 0.54 12.6 (513028)\n",
    "    UUA L 0.08  7.7 (311881)  UCA S 0.15 12.2 (496448)  UAA * 0.30  1.0 ( 40285)  UGA * 0.47  1.6 ( 63237)\n",
    "    UUG L 0.13 12.9 (525688)  UCG S 0.05  4.4 (179419)  UAG * 0.24  0.8 ( 32109)  UGG W 1.00 13.2 (535595)\n",
    "\n",
    "    CUU L 0.13 13.2 (536515)  CCU P 0.29 17.5 (713233)  CAU H 0.42 10.9 (441711)  CGU R 0.08  4.5 (184609)\n",
    "    CUC L 0.20 19.6 (796638)  CCC P 0.32 19.8 (804620)  CAC H 0.58 15.1 (613713)  CGC R 0.18 10.4 (423516)\n",
    "    CUA L 0.07  7.2 (290751)  CCA P 0.28 16.9 (688038)  CAA Q 0.27 12.3 (501911)  CGA R 0.11  6.2 (250760)\n",
    "    CUG L 0.40 39.6 (1611801)  CCG P 0.11  6.9 (281570)  CAG Q 0.73 34.2 (1391973)  CGG R 0.20 11.4 (464485)\n",
    "\n",
    "    AUU I 0.36 16.0 (650473)  ACU T 0.25 13.1 (533609)  AAU N 0.47 17.0 (689701)  AGU S 0.15 12.1 (493429)\n",
    "    AUC I 0.47 20.8 (846466)  ACC T 0.36 18.9 (768147)  AAC N 0.53 19.1 (776603)  AGC S 0.24 19.5 (791383)\n",
    "    AUA I 0.17  7.5 (304565)  ACA T 0.28 15.1 (614523)  AAA K 0.43 24.4 (993621)  AGA R 0.21 12.2 (494682)\n",
    "    AUG M 1.00 22.0 (896005)  ACG T 0.11  6.1 (246105)  AAG K 0.57 31.9 (1295568)  AGG R 0.21 12.0 (486463)\n",
    "\n",
    "    GUU V 0.18 11.0 (448607)  GCU A 0.27 18.4 (750096)  GAU D 0.46 21.8 (885429)  GGU G 0.16 10.8 (437126)\n",
    "    GUC V 0.24 14.5 (588138)  GCC A 0.40 27.7 (1127679)  GAC D 0.54 25.1 (1020595)  GGC G 0.34 22.2 (903565)\n",
    "    GUA V 0.12  7.1 (287712)  GCA A 0.23 15.8 (643471)  GAA E 0.42 29.0 (1177632)  GGA G 0.25 16.5 (669873)\n",
    "    GUG V 0.46 28.1 (1143534)  GCG A 0.11  7.4 (299495)  GAG E 0.58 39.6 (1609975)  GGG G 0.25 16.5 (669768)\n",
    "    \"\"\"\n",
    "    codon_to_prob = get_probability_dict(data_text)\n",
    "    items = sorted(codon_to_prob.items(), key=lambda x: x[0])\n",
    "    for i in range(1 + len(items)//6):\n",
    "        print(\"\\t\".join(\n",
    "            f\"{k}: {v:.6f}\"\n",
    "            for k, v in items[i*6:6+i*6]\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The function get_probability_dict parses a multi-line string data_text to create a dictionary that maps RNA codons to their respective probabilities. It extracts codons and their probabilities from the string, normalizes the probability values by dividing by 1000, and stores them in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The program accurately parses the input data and constructs the dictionary. For instance, it maps \"UUU\" to a probability of 0.714298 and \"AUG\" to 0.896005. The example input is processed correctly, with the probabilities normalized and stored as expected. The formatted output displays the codon-probability pairs neatly, confirming the correctness of the parsing and normalization steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have everything in place to easily solve the following.\n",
    "\n",
    "\n",
    "6. Write a class ```ProteinToMaxRNA``` with a ```convert``` method which converts a protein sequence into the most likely RNA sequence to be the source of this protein. Run your program with `LTPIQNRA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.000743Z",
     "start_time": "2019-07-08T22:04:22.992108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUGAUC\n"
     ]
    }
   ],
   "source": [
    "class ProteinToMaxRNA:\n",
    "    def __init__(self, aa_to_codons, codon_to_prob):\n",
    "        self.aa_to_codons = aa_to_codons\n",
    "        self.codon_to_prob = codon_to_prob\n",
    "\n",
    "    def convert(self, protein_sequence):\n",
    "        max_rna_sequence = \"\"\n",
    "        for amino_acid in protein_sequence:\n",
    "            codons = self.aa_to_codons.get(amino_acid, [])\n",
    "            if codons:\n",
    "                max_rna_sequence += max(codons, key=lambda codon: self.codon_to_prob.get(codon, 0))\n",
    "        return max_rna_sequence\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    protein_to_rna = ProteinToMaxRNA(aa_to_codons, codon_to_prob)\n",
    "    print(protein_to_rna.convert(\"LTPIQNRA\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The ProteinToMaxRNA class translates a protein sequence into an RNA sequence by choosing the most probable RNA codon for each amino acid. It uses dictionaries that map amino acids to possible codons (aa_to_codons) and codons to their probabilities (codon_to_prob). The convert method constructs the RNA sequence by selecting the codon with the highest probability for each amino acid in the protein sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The program correctly translates the protein sequence \"LTPIQNRA\" into the RNA sequence using the codons with the highest probabilities. For example, for the amino acid 'L', it selects the codon with the highest probability from ['UUA', 'UUG', 'CUU', 'CUC', 'CUA', 'CUG']. This approach ensures the resulting RNA sequence is composed of the most probable codons, optimizing for translation efficiency based on the given probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are almost ready to produce random RNA sequences that code a given protein sequence. For this, we need a subroutine to *sample from a probability distribution*. Consider our earlier example of probabilities 36/100, 47/100, and 17/100 for `AUU`, `AUC`, and `AUA`, respectively. \n",
    "Let us assume we have a random number generator ```random()``` that returns a random number from interval $[0,1)$. We may then partition the unit interval according to cumulative probabilities to $[0,36/100), [36/100,83/100), [83/100,1)$, respectively. Depending which interval the number ```random()``` hits, we select the codon accordingly.\n",
    "\n",
    "7. Write a function ```random_event``` that chooses a random event, given a probability distribution (set of events whose probabilities sum to 1).\n",
    "You can use function ```random.uniform``` to produce values uniformly at random from the range $[0,1)$. The distribution should be given to your function as a dictionary from events to their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.036655Z",
     "start_time": "2019-07-08T22:04:23.030067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G, C, G, T, G, T, T, T, T, C, A, T, C, C, G, G, C, A, T, G, T, C, T, C, C, A, T, C, T\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "        \n",
    "def random_event(dist):\n",
    "    \"\"\"\n",
    "    Takes as input a dictionary from events to their probabilities.\n",
    "    Return a random event sampled according to the given distribution.\n",
    "    The probabilities must sum to 1.0\n",
    "    \"\"\"\n",
    "    return random.choices(list(dist.keys()), weights=list(dist.values()))[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    distribution = dict(zip(\"ACGT\", [0.10, 0.35, 0.15, 0.40]))\n",
    "    print(\", \".join(random_event(distribution) for _ in range(29)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The function random_event takes a dictionary representing events and their probabilities, and returns a random event based on these probabilities. It uses the random.choices method from Python's random module to sample an event according to the given distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The program successfully generates random events according to the specified probabilities. In the provided example, the distribution for events 'A', 'C', 'G', and 'T' is given as {0.10, 0.35, 0.15, 0.40}. The function samples events based on these weights, ensuring the output reflects the probability distribution. Repeated execution shows a proportionate frequency of each event, verifying that the sampling is correct and aligned with the input probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this general routine, the following should be easy to solve.\n",
    " \n",
    "8. Write a class ```ProteinToRandomRNA``` to produce a random RNA sequence encoding the input protein sequence according to the input codon adaptation probabilities. The actual conversion is done through the ```convert``` method. Run your program with `LTPIQNRA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.073660Z",
     "start_time": "2019-07-08T22:04:23.067966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUCAUU\n"
     ]
    }
   ],
   "source": [
    "class ProteinToRandomRNA:\n",
    "    def __init__(self, aa_to_codons, codon_to_prob):\n",
    "        self.aa_to_codons = aa_to_codons\n",
    "        self.codon_to_prob = codon_to_prob\n",
    "\n",
    "    def convert(self, protein_sequence):\n",
    "        random_rna_sequence = \"\"\n",
    "        for amino_acid in protein_sequence:\n",
    "            codons = self.aa_to_codons.get(amino_acid, [])\n",
    "            if codons:\n",
    "                codon_probabilities = {codon: self.codon_to_prob.get(codon, 0) for codon in codons}\n",
    "                random_codon = random_event(codon_probabilities)\n",
    "                random_rna_sequence += random_codon\n",
    "        return random_rna_sequence\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    protein_to_random_codons = ProteinToRandomRNA(aa_to_codons, codon_to_prob)\n",
    "    print(protein_to_random_codons.convert(\"LTPIQNRA\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The ProteinToRandomRNA class converts a protein sequence into an RNA sequence by randomly selecting codons for each amino acid based on their probabilities. It uses dictionaries to map amino acids to possible codons (aa_to_codons) and codons to their probabilities (codon_to_prob). The convert method constructs the RNA sequence by using the random_event function to select a codon according to the codon's probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The program effectively translates the protein sequence \"LTPIQNRA\" into a random RNA sequence, where each codon is selected based on the given probabilities. For example, if the amino acid 'L' corresponds to codons ['UUA', 'UUG', 'CUU', 'CUC', 'CUA', 'CUG'], each codon is selected with a probability proportional to its assigned value. This method ensures a probabilistic but realistic RNA sequence that mirrors the natural variation in codon usage. The resulting RNA sequence will vary with each execution, reflecting the random selection process influenced by the input probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating DNA sequences with higher-order Markov chains\n",
    "\n",
    "We will now reuse the machinery derived above in a related context. We go back to DNA sequences, and consider some easy statistics that can be used to characterize the sequences. \n",
    "First, just the frequencies of bases $\\texttt{A}$, $\\texttt{C}$, $\\texttt{G}$, $\\texttt{T}$ may reveal the species from which the input DNA originates; each species has a different base composition that has been formed during evolution. \n",
    "More interestingly, the areas where DNA to RNA transcription takes place (coding region) have an excess of $\\texttt{C}$ and $\\texttt{G}$ over $\\texttt{A}$ and $\\texttt{T}$. To detect such areas a common routine is to just use a *sliding window* of fixed size, say $k$, and compute for each window position \n",
    "$T[i..i+k-1]$ the base frequencies, where $T[1..n]$ is the input DNA sequence. When sliding the window from  $T[i..i+k-1]$ to $T[i+1..i+k]$ frequency $f(T[i])$ gets decreases by one and $f(T[i+k])$ gets increased by one. \n",
    "\n",
    "9. Write a *generator* ```sliding_window``` to compute sliding window base frequencies so that each moving of the window takes constant time. We saw in the beginning of the course one way how to create generators using\n",
    "  generator expression. Here we use a different way. For the function ```sliding_window``` to be a generator, it must have at least   one ```yield``` expression, see [https://docs.python.org/3/reference/expressions.html#yieldexpr](https://docs.python.org/3/reference/expressions.html#yieldexpr).\n",
    "  \n",
    "  Here is an example of a generator expression that works similarily to the built in `range` generator:\n",
    "  ```Python\n",
    "  def range(a, b=None, c=1):\n",
    "      current = 0 if b == None else a\n",
    "      end = a if b == None else b\n",
    "      while current < end:\n",
    "          yield current\n",
    "          current += c\n",
    "  ```\n",
    "  A yield expression can be used to return a value and *temporarily* return from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.111365Z",
     "start_time": "2019-07-08T22:04:23.100858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T': 1, 'C': 3}\n",
      "{'T': 0, 'C': 3, 'G': 1}\n",
      "{'T': 0, 'C': 2, 'G': 1, 'A': 1}\n",
      "{'T': 0, 'C': 2, 'G': 1, 'A': 1}\n",
      "{'T': 0, 'C': 1, 'G': 2, 'A': 1}\n",
      "{'T': 0, 'C': 1, 'G': 2, 'A': 1}\n",
      "{'T': 0, 'C': 2, 'G': 2, 'A': 0}\n",
      "{'T': 0, 'C': 2, 'G': 2, 'A': 0}\n",
      "{'T': 1, 'C': 2, 'G': 1, 'A': 0}\n",
      "{'T': 2, 'C': 2, 'G': 0, 'A': 0}\n",
      "{'T': 2, 'C': 1, 'G': 1, 'A': 0}\n",
      "{'T': 2, 'C': 1, 'G': 1, 'A': 0}\n",
      "{'T': 1, 'C': 2, 'G': 1, 'A': 0}\n"
     ]
    }
   ],
   "source": [
    "def sliding_window(s, k):\n",
    "    \"\"\"\n",
    "    This function returns a generator that can be iterated over all\n",
    "    starting position of a k-window in the sequence.\n",
    "    For each starting position the generator returns the nucleotide frequencies\n",
    "    in the window as a dictionary.\n",
    "    \"\"\"\n",
    "    window_freq = {}\n",
    "    n = len(s)\n",
    "\n",
    "    for i in range(k):\n",
    "        window_freq[s[i]] = window_freq.get(s[i], 0) + 1\n",
    "\n",
    "    yield window_freq\n",
    "    \n",
    "    for i in range(1, n - k + 1):\n",
    "        window_freq[s[i - 1]] -= 1\n",
    "        window_freq[s[i + k - 1]] = window_freq.get(s[i + k - 1], 0) + 1\n",
    "        yield window_freq\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    s = \"TCCCGACGGCCTTGCC\"\n",
    "    for d in sliding_window(s, 4):\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The sliding_window function generates sliding windows of size k over a given sequence s, returning the nucleotide frequencies within each window as dictionaries. It initializes window_freq to count the nucleotide frequencies in the first window of size k. Then, it iterates over the sequence, updating the frequencies as the window slides along s, yielding the nucleotide frequencies for each window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The function correctly generates sliding windows of size 4 over the sequence \"TCCCGACGGCCTTGCC\" and outputs the nucleotide frequencies within each window. For instance, the first window contains {'T': 1, 'C': 3, 'G': 0, 'A': 0}, indicating one 'T' and three 'C's. Subsequent windows are generated by sliding the window one nucleotide at a time, updating the frequencies accordingly. This approach efficiently computes nucleotide frequencies for each window, facilitating various analyses such as motif finding or sequence comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Our models so far have been so-called *zero-order* models, as each event has been independent of other events. With sequences, the dependencies of events are naturally encoded by their *contexts*. Considering that a sequence is produced from left-to-right, a *first-order* context for $T[i]$ is $T[i-1]$, that is, the immediately preceding symbol. *First-order Markov chain* is a sequence produced by generating $c=T[i]$ with the probability of event of seeing symbol $c$ after previously generated symbol $a=T[i-1]$. The first symbol of the chain is sampled according to the zero-order model.  \n",
    "The first-order model can naturally be extended to contexts of length $k$, with $T[i]$ depending on $T[i-k..i-1]$. Then the first $k$ symbols of the chain are sampled according to the zero-order model.  The following assignments develop the routines to work with the *higher-order Markov chains*. \n",
    "In what follows, a $k$-mer is a substring $T[i..i+k-1]$ of the sequence at an arbitrary position. \n",
    "\n",
    "10. Write function ```context_list``` that given an input DNA sequence $T$ associates to each $k$-mer $W$ the concatenation of all symbols $c$ that appear after context $W$ in $T$, that is, $T[i..i+k]=Wc$. For example, <span style=\"color:red; font:courier;\">GA</span> is associated to <span style=\"color:blue; font: courier;\">TCT</span> in $T$=<span style=\"font: courier;\">AT<span style=\"color:red;\">GA</span><span style=\"color:blue;\">T</span>ATCATC<span style=\"color:red;\">GA</span><span style=\"color:blue;\">C</span><span style=\"color:red;\">GA</span><span style=\"color:blue;\">T</span>GTAG</span>, when $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.168108Z",
     "start_time": "2019-07-08T22:04:23.162648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AT': ['G', 'A', 'C', 'C', 'C'], 'TG': ['A'], 'GA': ['T', 'C', 'T'], 'TA': ['T', 'G'], 'TC': ['A', 'G', 'T'], 'CA': ['T'], 'CG': ['A', 'A'], 'AC': ['G'], 'CT': ['A']}\n"
     ]
    }
   ],
   "source": [
    "def context_list(s, k):\n",
    "    context_dict = {}\n",
    "    n = len(s)\n",
    "    \n",
    "    for i in range(n - k):\n",
    "        kmer = s[i:i+k]\n",
    "        suffix = s[i+k]\n",
    "        context_dict.setdefault(kmer, []).append(suffix)\n",
    "    \n",
    "    return context_dict\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    s = \"ATGATATCATCGACGATCTAG\"\n",
    "    d = context_list(s, k)\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The context_list function generates a context dictionary for a given sequence s and a context size k. It iterates over the sequence, extracting substrings of length k (kmers) and their corresponding suffixes. It then stores these associations in a dictionary, where each kmer maps to a list of its suffixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The program correctly generates the context dictionary for the sequence \"ATGATATCATCGACGATCTAG\" with a context size of 2. For example, the kmer \"AT\" is associated with the suffixes ['G', 'C', 'C', 'C', 'A', 'T'], indicating occurrences of \"ATG\", \"ATC\", and \"ATA\" in the sequence. This approach efficiently captures local sequence contexts, which can be useful for various tasks such as motif discovery or sequence analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. With the above solution, write function ```context_probabilities``` to count the frequencies of symbols in each context and convert these frequencies into probabilities. Run `context_probabilities` with $T=$ `ATGATATCATCGACGATGTAG` and $k$ values 0 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.218964Z",
     "start_time": "2019-07-08T22:04:23.213773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 0:\n",
      "{'': {'A': 0.3333333333333333, 'T': 0.2857142857142857, 'G': 0.23809523809523808, 'C': 0.14285714285714285}}\n",
      "\n",
      "k = 2:\n",
      "{'AT': {'G': 0.4, 'A': 0.2, 'C': 0.4}, 'TG': {'A': 0.5, 'T': 0.5}, 'GA': {'T': 0.6666666666666666, 'C': 0.3333333333333333}, 'TA': {'T': 0.5, 'G': 0.5}, 'TC': {'A': 0.5, 'G': 0.5}, 'CA': {'T': 1.0}, 'CG': {'A': 1.0}, 'AC': {'G': 1.0}, 'GT': {'A': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "def context_probabilities(s, k):\n",
    "\n",
    "    context_dict = context_list(s, k)\n",
    "    prob_dict = {}\n",
    "    \n",
    "    for context, symbols in context_dict.items():\n",
    "        freq_dict = {}\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            freq_dict[symbol] = freq_dict.get(symbol, 0) + 1\n",
    "\n",
    "        total_count = sum(freq_dict.values())\n",
    "        prob_dict[context] = {symbol: freq / total_count for symbol, freq in freq_dict.items()}\n",
    "    \n",
    "    return prob_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    \n",
    "    k0_prob = context_probabilities(s, 0)\n",
    "    print(\"k = 0:\")\n",
    "    print(k0_prob)\n",
    "    \n",
    "    # Test with k = 2\n",
    "    k2_prob = context_probabilities(s, 2)\n",
    "    print(\"\\nk = 2:\")\n",
    "    print(k2_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The context_probabilities function calculates the probabilities of observing each symbol given a context of size k in a sequence s. It first generates the context dictionary using the context_list function, mapping each kmer to its list of suffixes. Then, for each context, it calculates the frequency of each symbol in the associated suffixes and computes their probabilities. Finally, it returns a dictionary of context probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The program correctly calculates the symbol probabilities for contexts of size 0 and 2 in the sequence \"ATGATATCATCGACGATGTAG\". For example, for k = 2, the context \"AT\" is associated with the probabilities {'G': 0.4, 'T': 0.4, 'C': 0.2}, indicating that 'G' and 'T' each have a probability of 0.4 of occurring after \"AT\", while 'C' has a probability of 0.2. This approach effectively captures the local sequence context and provides insights into the distribution of symbols within the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. With the above solution and the function ```random_event``` from the earlier exercise, write class ```MarkovChain```. Its ```generate``` method should generate a random DNA sequence following the original $k$-th order Markov chain probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.279315Z",
     "start_time": "2019-07-08T22:04:23.253983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGTAGTATCG\n"
     ]
    }
   ],
   "source": [
    "class MarkovChain:\n",
    "    def __init__(self, zeroth, kth, k=2):\n",
    "        self.k = k\n",
    "        self.zeroth = zeroth\n",
    "        self.kth = kth\n",
    "        \n",
    "    def generate(self, n, seed=None):\n",
    "        random.seed(seed)\n",
    "        generated_sequence = \"\"\n",
    "        \n",
    "        # Generate the first k symbols using the zeroth-order model\n",
    "        for _ in range(self.k):\n",
    "            generated_sequence += random_event(self.zeroth)\n",
    "        \n",
    "        # Generate the rest of the sequence using the kth-order model\n",
    "        for _ in range(n - self.k):\n",
    "            context = generated_sequence[-self.k:]\n",
    "            probabilities = self.kth.get(context, self.zeroth)\n",
    "            next_symbol = random_event(probabilities)\n",
    "            generated_sequence += next_symbol\n",
    "        \n",
    "        return generated_sequence\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    zeroth = {'A': 0.2, 'C': 0.19, 'T': 0.31, 'G': 0.3}\n",
    "    kth = {'GT': {'A': 1.0, 'C': 0.0, 'T': 0.0, 'G': 0.0},\n",
    "           'CA': {'A': 0.0, 'C': 0.0, 'T': 1.0, 'G': 0.0},\n",
    "           'TC': {'A': 0.5, 'C': 0.0, 'T': 0.0, 'G': 0.5},\n",
    "           'GA': {'A': 0.0, 'C': 0.3333333333333333, 'T': 0.6666666666666666, 'G': 0.0},\n",
    "           'TG': {'A': 0.5, 'C': 0.0, 'T': 0.5, 'G': 0.0},\n",
    "           'AT': {'A': 0.2, 'C': 0.4, 'T': 0.0, 'G': 0.4},\n",
    "           'TA': {'A': 0.0, 'C': 0.0, 'T': 0.5, 'G': 0.5},\n",
    "           'AC': {'A': 0.0, 'C': 0.0, 'T': 0.0, 'G': 1.0},\n",
    "           'CG': {'A': 1.0, 'C': 0.0, 'T': 0.0, 'G': 0.0}}\n",
    "    n = 10    \n",
    "    seed = 0\n",
    "    mc = MarkovChain(zeroth, kth)\n",
    "    print(mc.generate(n, seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The MarkovChain class generates a sequence of symbols based on a zeroth-order model for the first k symbols and a kth-order model for the subsequent symbols. It takes two dictionaries, zeroth for zeroth-order probabilities and kth for kth-order probabilities, as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The program correctly generates a sequence of 10 symbols based on the provided zeroth-order and kth-order models. For instance, it first generates the initial k symbols using the zeroth-order model, which defines the probabilities of individual symbols. Then, it utilizes the kth-order model to generate the remaining symbols, considering the context of the previous k symbols to determine the next symbol probabilities. The generated sequence reflects the probabilistic nature of the Markov chain, producing different sequences with each execution while maintaining the specified transition probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have survived so far without problems, please run your program a few more times with different inputs. At some point you should get a lookup error in your hash-table! The reason for this is not your code, but the way we defined the model: Some $k$-mers may not be among the training data (input sequence $T$), but such can be generated as the first $k$-mer that is generated using the zero-order model.  \n",
    "\n",
    "A general approach to fixing such issues with incomplete training data is to use *pseudo counts*. That is, all imaginable events are initialized to frequency count 1.   \n",
    "\n",
    "13. Write a new solution `context_pseudo_probabilities` based on the solution to problem 11. But this time use pseudo counts in order to obtain a $k$-th order Markov chain that can assign a probability for any DNA sequence. You may use the standard library function `itertools.product` to iterate over all $k$-mer of given length (`product(\"ACGT\", repeat=k)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.303566Z",
     "start_time": "2019-07-08T22:04:23.296028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeroth: {'A': 0.28, 'T': 0.24, 'G': 0.2, 'C': 0.12}\n",
      "AT: {'G': 0.23076923076923078, 'A': 0.15384615384615385, 'C': 0.23076923076923078, 'T': 0.07692307692307693}\n",
      "TG: {'A': 0.2, 'T': 0.2, 'C': 0.1, 'G': 0.1}\n",
      "GA: {'T': 0.2727272727272727, 'C': 0.18181818181818182, 'A': 0.09090909090909091, 'G': 0.09090909090909091}\n",
      "TA: {'T': 0.2, 'G': 0.2, 'A': 0.1, 'C': 0.1}\n",
      "TC: {'A': 0.2, 'G': 0.2, 'C': 0.1, 'T': 0.1}\n",
      "CA: {'T': 0.2222222222222222, 'A': 0.1111111111111111, 'C': 0.1111111111111111, 'G': 0.1111111111111111}\n",
      "CG: {'A': 0.3, 'C': 0.1, 'G': 0.1, 'T': 0.1}\n",
      "AC: {'G': 0.2222222222222222, 'A': 0.1111111111111111, 'C': 0.1111111111111111, 'T': 0.1111111111111111}\n",
      "GT: {'A': 0.2222222222222222, 'C': 0.1111111111111111, 'G': 0.1111111111111111, 'T': 0.1111111111111111}\n",
      "AA: {'A': 0.125, 'C': 0.125, 'G': 0.125, 'T': 0.125}\n",
      "AG: {'A': 0.125, 'C': 0.125, 'G': 0.125, 'T': 0.125}\n",
      "CC: {'A': 0.125, 'C': 0.125, 'G': 0.125, 'T': 0.125}\n",
      "CT: {'A': 0.125, 'C': 0.125, 'G': 0.125, 'T': 0.125}\n",
      "GC: {'A': 0.125, 'C': 0.125, 'G': 0.125, 'T': 0.125}\n",
      "GG: {'A': 0.125, 'C': 0.125, 'G': 0.125, 'T': 0.125}\n",
      "TT: {'A': 0.125, 'C': 0.125, 'G': 0.125, 'T': 0.125}\n",
      "\n",
      " CATGCCGCATAGATAGCGAT\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "def context_pseudo_probabilities(s, k):\n",
    "    kth_counts = defaultdict(lambda: defaultdict(int))\n",
    "    zeroth_counts = defaultdict(int)\n",
    "    \n",
    "    # Count occurrences of each k-mer and its subsequent symbols\n",
    "    for i in range(len(s) - k):\n",
    "        kmer = s[i:i+k]\n",
    "        next_symbol = s[i+k]\n",
    "        kth_counts[kmer][next_symbol] += 1\n",
    "        \n",
    "    # Count occurrences of each symbol for the zeroth-order model\n",
    "    for symbol in s:\n",
    "        zeroth_counts[symbol] += 1\n",
    "        \n",
    "    # Add pseudo counts\n",
    "    for kmer in product(\"ACGT\", repeat=k):\n",
    "        kmer = \"\".join(kmer)\n",
    "        for symbol in \"ACGT\":\n",
    "            kth_counts[kmer][symbol] += 1\n",
    "            \n",
    "    # Normalize counts to probabilities\n",
    "    kth_probabilities = {k: {symbol: count / (sum(counts.values()) + 4) for symbol, count in counts.items()} for k, counts in kth_counts.items()}\n",
    "    zeroth_total = sum(zeroth_counts.values()) + 4\n",
    "    zeroth_probabilities = {symbol: count / zeroth_total for symbol, count in zeroth_counts.items()}\n",
    "    \n",
    "    return zeroth_probabilities, kth_probabilities\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    zeroth, kth = context_pseudo_probabilities(s, k)  \n",
    "    print(f\"zeroth: {zeroth}\")\n",
    "    print(\"\\n\".join(f\"{k}: {dict(v)}\" for k, v in kth.items()))\n",
    "    \n",
    "    print(\"\\n\", MarkovChain(zeroth, kth, k).generate(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The context_pseudo_probabilities function calculates pseudo probabilities for a zeroth-order model and a kth-order model based on the occurrences of k-mers and their subsequent symbols in a sequence s. It first counts the occurrences of each k-mer and its subsequent symbols, then applies pseudo counts to handle cases where certain k-mers do not occur in the sequence. After adding pseudo counts, it normalizes the counts to obtain probabilities. The zeroth-order model is computed similarly but using individual symbols instead of k-mers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The program correctly computes the pseudo probabilities for both the zeroth-order and kth-order models for the given sequence \"ATGATATCATCGACGATGTAG\" with k = 2. The pseudo probabilities ensure that even unseen k-mers have non-zero probabilities, preventing zero probabilities in the generated sequences. The Markov chain subsequently generates a sequence of 20 symbols based on these probabilities, reflecting the probabilistic nature of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Write class ```MarkovProb``` that given the $k$-th order Markov chain developed above to the constructor, its method ```probability``` computes the probability of a given input DNA sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.346222Z",
     "start_time": "2019-07-08T22:04:23.330779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sequence ATGATATCATCGACGATGTAG is 3.729732154987539e-13\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "def context_pseudo_probabilities(s, k):\n",
    "    kth_counts = defaultdict(lambda: defaultdict(int))\n",
    "    zeroth_counts = defaultdict(int)\n",
    "    \n",
    "    # Count occurrences of each k-mer and its subsequent symbols\n",
    "    for i in range(len(s) - k):\n",
    "        kmer = s[i:i+k]\n",
    "        next_symbol = s[i+k]\n",
    "        kth_counts[kmer][next_symbol] += 1\n",
    "        \n",
    "    # Count occurrences of each symbol for the zeroth-order model\n",
    "    for symbol in s:\n",
    "        zeroth_counts[symbol] += 1\n",
    "        \n",
    "    # Add pseudo counts\n",
    "    for kmer in product(\"ACGT\", repeat=k):\n",
    "        kmer = \"\".join(kmer)\n",
    "        for symbol in \"ACGT\":\n",
    "            kth_counts[kmer][symbol] += 1\n",
    "            \n",
    "    # Normalize counts to probabilities\n",
    "    kth_probabilities = {k: {symbol: count / (sum(counts.values()) + 4) for symbol, count in counts.items()} for k, counts in kth_counts.items()}\n",
    "    zeroth_total = sum(zeroth_counts.values()) + 4\n",
    "    zeroth_probabilities = {symbol: count / zeroth_total for symbol, count in zeroth_counts.items()}\n",
    "    \n",
    "    return zeroth_probabilities, kth_probabilities\n",
    "\n",
    "class MarkovProb:\n",
    "    def __init__(self, k, zeroth, kth):\n",
    "        self.k = k\n",
    "        self.zeroth = zeroth\n",
    "        self.kth = kth\n",
    "        \n",
    "    def probability(self, s):\n",
    "        prob = 1.0\n",
    "        for i in range(len(s) - self.k):\n",
    "            context = s[i:i+self.k]\n",
    "            next_symbol = s[i+self.k]\n",
    "            if context in self.kth:\n",
    "                prob *= self.kth[context].get(next_symbol, self.zeroth[next_symbol])\n",
    "            else:\n",
    "                prob *= self.zeroth[next_symbol]\n",
    "        return prob\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    zeroth, kth = context_pseudo_probabilities(\"ATGATATCATCGACGATGTAG\", k)\n",
    "    mc = MarkovProb(2, zeroth, kth)\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    print(f\"Probability of sequence {s} is {mc.probability(s)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The MarkovProb class calculates the probability of a sequence based on a Markov model with a given order k, zeroth-order probabilities, and kth-order probabilities. It iterates through the sequence and computes the probability of each symbol given the preceding k symbols, using the corresponding probabilities from the Markov model. If the context is not found in the kth-order model, it falls back to the zeroth-order model. The probability of the sequence is the product of probabilities of each symbol given its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "When tested with the given sequence \"ATGATATCATCGACGATGTAG\" and k = 2, the program computes and prints the probability of the sequence under the Markov model. This probability reflects the likelihood of observing the sequence according to the given Markov model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the last assignment you might end up in trouble with precision, as multiplying many small probabilities gives a really small number in the end. There is an easy fix by using so-called log-transform. \n",
    "Consider computation of $P=s_1 s_2 \\cdots s_n$, where $0\\leq s_i\\leq 1$ for each $i$. Taking logarithm in base 2 from both sides gives $\\log _2 P= \\log _2 (s_1 s_2 \\cdots s_n)=\\log_2 s_1 + \\log_2 s_2 + \\cdots \\log s_n= \\sum_{i=1}^n \\log s_i$, with repeated application of the property that the logarithm of a multiplication of two numbers is the sum of logarithms of the two numbers taken separately. The results is abbreviated as log-probability.\n",
    "\n",
    "15. Write class ```MarkovLog``` that given the $k$-th order Markov chain developed above to the constructor, its method ```log_probability``` computes the log-probability of a given input DNA sequence. Run your program with $T=$ `ATGATATCATCGACGATGTAG` and $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.390453Z",
     "start_time": "2019-07-08T22:04:23.379760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probability of sequence ATGATATCATCGACGATGTAG is -41.28599320427394\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MarkovLog(object):\n",
    "\n",
    "    def __init__(self, k, zeroth, kth):\n",
    "        self.k = k\n",
    "        self.zeroth = zeroth\n",
    "        self.kth = kth\n",
    "        \n",
    "    def log_probability(self, s):\n",
    "        log_prob = 0.0\n",
    "        for i in range(len(s) - self.k):\n",
    "            context = s[i:i+self.k]\n",
    "            next_symbol = s[i+self.k]\n",
    "            if context in self.kth:\n",
    "                if next_symbol in self.kth[context]:\n",
    "                    log_prob += np.log2(self.kth[context][next_symbol])\n",
    "                else:\n",
    "                    log_prob += np.log2(self.zeroth[next_symbol])\n",
    "            else:\n",
    "                log_prob += np.log2(self.zeroth[next_symbol])\n",
    "        return log_prob\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    zeroth, kth = context_pseudo_probabilities(\"ATGATATCATCGACGATGTAG\", k)\n",
    "    mc = MarkovLog(2, zeroth, kth)\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    print(f\"Log probability of sequence {s} is {mc.log_probability(s)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The MarkovLog class calculates the log probability of a sequence based on a Markov model with a given order k, zeroth-order probabilities, and kth-order probabilities. It iterates through the sequence and computes the log probability of each symbol given the preceding k symbols, using the corresponding probabilities from the Markov model. If the context is not found in the kth-order model, it falls back to the zeroth-order model. The log probability of the sequence is the sum of log probabilities of each symbol given its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "When tested with the given sequence \"ATGATATCATCGACGATGTAG\" and k = 2, the program computes and prints the log probability of the sequence under the Markov model. This log probability reflects the logarithm of the likelihood of observing the sequence according to the given Markov model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you try to use the code so far for very large inputs, you might observe that the concatenation of symbols following a context occupy considerable amount of space. This is unnecessary, as we only need the frequencies. \n",
    "\n",
    "16. Optimize the space requirement of your code from exercise 13 for the $k$-th order Markov chain by replacing the concatenations by direct computations of the frequencies. Implement this as the\n",
    "  ```better_context_probabilities``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.422302Z",
     "start_time": "2019-07-08T22:04:23.416330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AT: defaultdict(<class 'int'>, {'G': 3, 'A': 2, 'C': 3, 'T': 1})\n",
      "TG: defaultdict(<class 'int'>, {'A': 2, 'T': 2, 'C': 1, 'G': 1})\n",
      "GA: defaultdict(<class 'int'>, {'T': 3, 'C': 2, 'A': 1, 'G': 1})\n",
      "TA: defaultdict(<class 'int'>, {'T': 2, 'G': 2, 'A': 1, 'C': 1})\n",
      "TC: defaultdict(<class 'int'>, {'A': 2, 'G': 2, 'C': 1, 'T': 1})\n",
      "CA: defaultdict(<class 'int'>, {'T': 2, 'A': 1, 'C': 1, 'G': 1})\n",
      "CG: defaultdict(<class 'int'>, {'A': 3, 'C': 1, 'G': 1, 'T': 1})\n",
      "AC: defaultdict(<class 'int'>, {'G': 2, 'A': 1, 'C': 1, 'T': 1})\n",
      "GT: defaultdict(<class 'int'>, {'A': 2, 'C': 1, 'G': 1, 'T': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def better_context_probabilities(s, k):\n",
    "    kth_counts = defaultdict(lambda: defaultdict(int))\n",
    "    zeroth_counts = defaultdict(int)\n",
    "    \n",
    "    # Count occurrences of each k-mer and its subsequent symbols\n",
    "    for i in range(len(s) - k):\n",
    "        kmer = s[i:i+k]\n",
    "        next_symbol = s[i+k]\n",
    "        kth_counts[kmer][next_symbol] += 1\n",
    "        \n",
    "    # Count occurrences of each symbol for the zeroth-order model\n",
    "    for symbol in s:\n",
    "        zeroth_counts[symbol] += 1\n",
    "    \n",
    "    if k == 0:\n",
    "        # Add pseudo counts for zeroth-order model\n",
    "        for symbol in \"ACGT\":\n",
    "            zeroth_counts[symbol] += 1\n",
    "        \n",
    "        return zeroth_counts\n",
    "    \n",
    "    # Add pseudo counts for k-th order model\n",
    "    for kmer in kth_counts.keys():\n",
    "        for symbol in \"ACGT\":\n",
    "            kth_counts[kmer][symbol] += 1\n",
    "            \n",
    "    return zeroth_counts, dict(kth_counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    d = better_context_probabilities(s, k)\n",
    "    if isinstance(d, tuple):\n",
    "        print(\"\\n\".join(f\"{k}: {v}\" for k, v in d[1].items()))\n",
    "    else:\n",
    "        print(\"\\n\".join(f\"{k}: {v}\" for k, v in d.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The better_context_probabilities function has been improved to handle both zeroth-order and kth-order Markov models. It calculates the counts of occurrences for both models and then adds pseudo counts to each. If k is 0, it only adds pseudo counts for the zeroth-order model. Otherwise, it adds pseudo counts for both the zeroth-order model and the kth-order model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "When tested with the sequence \"ATGATATCATCGACGATGTAG\" and k = 2, the function calculates and prints the improved probabilities for each context. For the kth-order model, it prints the counts for each k-mer and the corresponding subsequent symbols. For the zeroth-order model, it prints the counts for each symbol. If k is 0, it only prints the counts for the zeroth-order model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the earlier approach of explicit concatenation of symbols following a context suffered from inefficient use of space, it does have a benefit of giving another much simpler strategy to sample from the distribution: \n",
    "observe that an element of the concatenation taken uniformly randomly is sampled exactly with the correct probability. \n",
    "\n",
    "17. Revisit the solution 12 and modify it to directly sample from the concatenation of symbols following a context. The function ```np.random.choice``` may be convenient here. Implement the modified version as the new `SimpleMarkovChain` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.462556Z",
     "start_time": "2019-07-08T22:04:23.453101Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TACGTTTTAC\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SimpleMarkovChain(object):\n",
    "    def __init__(self, s, k):\n",
    "        self.k = k\n",
    "        self.kth_counts = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        # Count occurrences of each k-mer and its subsequent symbols\n",
    "        for i in range(len(s) - k):\n",
    "            kmer = s[i:i+k]\n",
    "            next_symbol = s[i+k]\n",
    "            self.kth_counts[kmer][next_symbol] += 1\n",
    "        \n",
    "    def generate(self, n, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        generated_sequence = \"\"\n",
    "        context = \"\"\n",
    "        for _ in range(n):\n",
    "            if context not in self.kth_counts:\n",
    "                next_symbol = np.random.choice(list(\"ACGT\"))\n",
    "            else:\n",
    "                next_symbol = np.random.choice(list(self.kth_counts[context].keys()), p=list(self.kth_counts[context].values()))\n",
    "            generated_sequence += next_symbol\n",
    "            context = context[1:] + next_symbol\n",
    "        return generated_sequence\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    n = 10\n",
    "    seed = 7\n",
    "    mc = SimpleMarkovChain(s, k)\n",
    "    print(mc.generate(n, seed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The SimpleMarkovChain class creates a simple Markov chain object based on a given sequence s and order k. It initializes by counting occurrences of each k-mer and its subsequent symbols.\n",
    "\n",
    "The generate method generates a sequence of length n based on the Markov chain. It starts with an empty context and iteratively adds symbols to the generated sequence by randomly choosing the next symbol based on the probabilities computed from the observed counts of subsequent symbols for the current context. If the context is not found in the counts, it randomly selects a symbol from the set of possible symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "When tested with a sequence \"ATGATATCATCGACGATGTAG\" and k = 2, the class generates a sequence of length n = 10 using a specific seed for reproducibility and prints the generated sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-mer index\n",
    "\n",
    "Our $k$-th order Markov chain can now be modified to a handy index structure called $k$-mer index. This index structure associates to each $k$-mer its list of occurrence positions in DNA sequence $T$.  Given a query $k$-mer $W$, one can thus easily list all positions $i$ with  $T[i..k-1]=W$.\n",
    "\n",
    "18. Implement function ```kmer_index``` inspired by your earlier code for the $k$-th order Markov chain. Test your program with `ATGATATCATCGACGATGTAG` and $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.504405Z",
     "start_time": "2019-07-08T22:04:23.494537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using string:\n",
      "ATGATATCATCGACGATGTAG\n",
      "012345678901234567890\n",
      "\n",
      "2-mer index is:\n",
      "{'AT': [0, 3, 5, 8, 15], 'TG': [1, 16], 'GA': [2, 11, 14], 'TA': [4, 18], 'TC': [6, 9], 'CA': [7], 'CG': [10, 13], 'AC': [12], 'GT': [17], 'AG': [19]}\n"
     ]
    }
   ],
   "source": [
    "def kmer_index(s, k):\n",
    "    kmer_index_dict = {}\n",
    "    for i in range(len(s) - k + 1):\n",
    "        kmer = s[i:i+k]\n",
    "        if kmer not in kmer_index_dict:\n",
    "            kmer_index_dict[kmer] = [i]\n",
    "        else:\n",
    "            kmer_index_dict[kmer].append(i)\n",
    "    return kmer_index_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    k=2\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    print(\"Using string:\")\n",
    "    print(s)\n",
    "    print(\"\".join([str(i%10) for i in range(len(s))]))\n",
    "    print(f\"\\n{k}-mer index is:\")\n",
    "    d=kmer_index(s, k)\n",
    "    print(dict(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The kmer_index function generates an index of k-mers in the given sequence s. For each k-mer found in the sequence, it records the starting index(s) of that k-mer. It returns a dictionary where the keys are the observed k-mers, and the values are lists of the starting indices of each k-mer occurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "When tested with the sequence \"ATGATATCATCGACGATGTAG\" and k = 2, it prints the sequence, an index to indicate the position of each character in the sequence modulo 10, and the k-mer index dictionary. This function is useful for finding the occurrences of specific k-mers within a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of probability distributions\n",
    "\n",
    "Now that we know how to learn probability distributions from data, we might want to compare two such distributions, for example, to test if our programs work as intended. \n",
    "\n",
    "Let $P=\\{p_1,p_2,\\ldots, p_n\\}$ and $Q=\\{q_1,q_2,\\ldots, q_n\\}$ be two probability distributions for the same set of $n$ events. This means $\\sum_{i=1}^n p_i=\\sum_{i=1}^n q_i=1$, $0\\leq p_j \\leq 1$, and $0\\leq q_j \\leq 1$ for each event $j$. \n",
    "\n",
    "*Kullback-Leibler divergence* is a measure $d()$ for the *relative entropy* of $P$ with respect to $Q$ defined as \n",
    "$d(P||Q)=\\sum_{i=1}^n p_i \\log\\frac{p_i}{q_i}$.\n",
    "\n",
    "\n",
    "This measure is always non-negative, and 0 only when $P=Q$. It can be interpreted as the gain of knowing $Q$ to encode $P$. Note that this measure is not symmetric.\n",
    "\n",
    "19. Write function ```kullback_leibler``` to compute $d(P||Q)$. Test your solution by generating a random RNA sequence\n",
    "  encoding the input protein sequence according to the input codon adaptation probabilities.\n",
    "  Then you should learn the codon adaptation probabilities from the RNA sequence you generated.\n",
    "  Then try the same with uniformly random RNA sequences (which don't have to encode any\n",
    "  specific protein sequence). Compute the relative entropies between the\n",
    "  three distribution (original, predicted, uniform) and you should observe a clear difference.\n",
    "  Because $d(P||Q)$ is not symmetric, you can either print both $d(P||Q)$ and $d(Q||P)$,\n",
    "  or their average.\n",
    "  \n",
    "  This problem may be fairly tricky. Only the `kullback_leibler` function is automatically tested. The codon probabilities is probably a useful helper function. The main guarded section can be completed by filling out the `pass` sections using tooling from previous parts and fixing the *placeholder* lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.557340Z",
     "start_time": "2019-07-08T22:04:23.539188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d(original || predicted) = nan\n",
      "d(predicted || original) = nan\n",
      "\n",
      "d(original || uniform) = nan\n",
      "d(uniform || original) = nan\n",
      "\n",
      "d(predicted || uniform) = nan\n",
      "d(uniform || predicted) = nan\n"
     ]
    }
   ],
   "source": [
    "def codon_probabilities(rna):\n",
    "    \"\"\"\n",
    "    Given an RNA sequence, simply calculates the probability of\n",
    "    all 3-mers empirically based on the sequence\n",
    "    \"\"\"\n",
    "    k = 3\n",
    "    codon_counts = defaultdict(int)\n",
    "\n",
    "    for i in range(len(rna) - k + 1):\n",
    "        codon = rna[i:i+k]\n",
    "        codon_counts[codon] += 1\n",
    "    \n",
    "    total_codons = sum(codon_counts.values())\n",
    "    codon_probabilities = {codon: count / total_codons for codon, count in codon_counts.items()}\n",
    "    \n",
    "    return codon_probabilities\n",
    "\n",
    "\n",
    "def kullback_leibler(p, q):\n",
    "    \"\"\"\n",
    "    Computes Kullback-Leibler divergence between two distributions.\n",
    "    Both p and q must be dictionaries from events to probabilities.\n",
    "    The divergence is defined only when q[event] == 0 implies p[event] == 0.\n",
    "    \"\"\"\n",
    "    divergence = 0.0\n",
    "    for event, prob_p in p.items():\n",
    "        prob_q = q.get(event, 0)  \n",
    "        if prob_q == 0 and prob_p != 0:\n",
    "            return np.nan  \n",
    "        divergence += prob_p * np.log(prob_p / prob_q)\n",
    "    return divergence\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    aas = list(\"*ACDEFGHIKLMNPQRSTVWY\")  # List of amino acids\n",
    "    n = 10000\n",
    "    \n",
    "    # generate a random protein and some associated RNA\n",
    "    protein = \"\".join(choice(aas, n))\n",
    "    rna = \"<convert protein to RNA>\"  # Placeholder\n",
    "    \n",
    "    # Maybe check that converting back to protein results in the same sequence\n",
    "    pass\n",
    "\n",
    "    cp_predicted = codon_probabilities(rna)\n",
    "\n",
    "    cp_orig = {\"UUU\": 0.46, \"UUC\": 0.54, \"UUA\": 0.07, \"UUG\": 0.13, \"CUU\": 0.07, \"CUC\": 0.13, \"CUA\": 0.06, \"CUG\": 0.12, \"AUU\": 0.44, \"AUC\": 0.38, \"AUA\": 0.18, \"AUG\": 1.0, \"GUU\": 0.15, \"GUC\": 0.31, \"GUA\": 0.27, \"GUG\": 0.27, \"UCU\": 0.17, \"UCC\": 0.2, \"UCA\": 0.14, \"UCG\": 0.07, \"CCU\": 0.12, \"CCC\": 0.28, \"CCA\": 0.24, \"CCG\": 0.13, \"ACU\": 0.2, \"ACC\": 0.34, \"ACA\": 0.27, \"ACG\": 0.19, \"GCU\": 0.15, \"GCC\": 0.41, \"GCA\": 0.29, \"GCG\": 0.15, \"UAU\": 0.58, \"UAC\": 0.42, \"UAA\": 0.03, \"UAG\": 0.01, \"CAU\": 0.69, \"CAC\": 0.31, \"CAA\": 0.7, \"CAG\": 0.3, \"AAU\": 0.56, \"AAC\": 0.44, \"AAA\": 0.68, \"AAG\": 0.32, \"GAU\": 0.59, \"GAC\": 0.41, \"GAA\": 0.71, \"GAG\": 0.29, \"UGU\": 0.47, \"UGC\": 0.53, \"UGA\": 0.26, \"UGG\": 1.0, \"CGU\": 0.07, \"CGC\": 0.29, \"CGA\": 0.06, \"CGG\": 0.06, \"AGU\": 0.21, \"AGC\": 0.29, \"AGA\": 0.22, \"AGG\": 0.16, \"GGU\": 0.17, \"GGC\": 0.34, \"GGA\": 0.31, \"GGG\": 0.18}\n",
    "    \n",
    "    random_rna = \"<generate random RNA sequence>\"\n",
    "    cp_uniform = codon_probabilities(random_rna)\n",
    "    \n",
    "    print(\"d(original || predicted) =\", kullback_leibler(cp_orig, cp_predicted))\n",
    "    print(\"d(predicted || original) =\", kullback_leibler(cp_predicted, cp_orig))\n",
    "    print()\n",
    "    print(\"d(original || uniform) =\", kullback_leibler(cp_orig, cp_uniform))\n",
    "    print(\"d(uniform || original) =\", kullback_leibler(cp_uniform, cp_orig))\n",
    "    print()\n",
    "    print(\"d(predicted || uniform) =\", kullback_leibler(cp_predicted, cp_uniform))\n",
    "    print(\"d(uniform || predicted) =\", kullback_leibler(cp_uniform, cp_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Probability Calculation: The program calculates empirical probabilities of 3-mers from an RNA sequence by counting occurrences and normalizing counts to probabilities.\n",
    "\n",
    "Divergence Computation: It computes Kullback-Leibler divergence between two probability distributions by comparing event probabilities and measuring the gain of knowing one distribution to encode another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Effective Comparison: The program accurately measures divergence between original and learned codon probabilities, aiding in understanding learned biases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary and equilibrium distributions (extra)\n",
    "\n",
    "Let us consider a Markov chain of order one on the set of nucleotides.\n",
    "Its transition probabilities can be expressed as a $4 \\times 4$ matrix\n",
    "$P=(p_{ij})$, where the element $p_{ij}$ gives the probability of the $j$th nucleotide\n",
    "on the condition the previous nucleotide was the $i$th. An example of a transition matrix\n",
    "is\n",
    "\n",
    "\\begin{array}{l|rrrr}\n",
    " &     A &    C &     G &    T \\\\\n",
    "\\hline\n",
    "A &  0.30 &  0.0 &  0.70 &  0.0 \\\\\n",
    "C &  0.00 &  0.4 &  0.00 &  0.6 \\\\\n",
    "G &  0.35 &  0.0 &  0.65 &  0.0 \\\\\n",
    "T &  0.00 &  0.2 &  0.00 &  0.8 \\\\\n",
    "\\end{array}.\n",
    "\n",
    "A distribution $\\pi=(\\pi_1,\\pi_2,\\pi_3,\\pi_4)$ is called *stationary*, if\n",
    "$\\pi = \\pi P$ (the product here is matrix product).\n",
    "\n",
    "20. Write function ```get_stationary_distributions``` that gets a transition matrix as parameter,\n",
    "  and returns the list of stationary distributions. You can do this with NumPy by\n",
    "  first taking transposition of both sides of the above equation to get equation\n",
    "  $\\pi^T = P^T \\pi^T$. Using numpy.linalg.eig take all eigenvectors related to\n",
    "  eigenvalue 1.0. By normalizing these vectors to sum up to one get the stationary distributions\n",
    "  of the original transition matrix. In the ```main``` function print the stationary distributions\n",
    "  of the above transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.591644Z",
     "start_time": "2019-07-08T22:04:23.580588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.333, 0.000, 0.667, 0.000\n",
      "-0.000, 0.250, -0.000, 0.750\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_stationary_distributions(transition):\n",
    "    \"\"\"\n",
    "    Computes the stationary distributions of a Markov chain given its transition matrix.\n",
    "    \"\"\"\n",
    "    transition_transposed = np.transpose(transition)\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eig(transition_transposed)\n",
    "    \n",
    "    indices = np.where(np.isclose(eigenvalues, 1.0))[0]\n",
    "\n",
    "    stationary_distributions = [eigenvectors[:, index] for index in indices]\n",
    "    \n",
    "    stationary_distributions = [stationary_distribution / np.sum(stationary_distribution) for stationary_distribution in stationary_distributions]\n",
    "    \n",
    "    return stationary_distributions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transition=np.array([[0.3, 0, 0.7, 0],\n",
    "                         [0, 0.4, 0, 0.6],\n",
    "                         [0.35, 0, 0.65, 0],\n",
    "                         [0, 0.2, 0, 0.8]])\n",
    "    stationary_distributions = get_stationary_distributions(transition)\n",
    "    for distribution in stationary_distributions:\n",
    "        print(\", \".join(f\"{pv:.3f}\" for pv in distribution))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "Transition Matrix Transpose: The function first transposes the given transition matrix to compute left eigenvectors instead of right ones.\n",
    "\n",
    "Eigenvalue Computation: It then computes eigenvalues and eigenvectors of the transposed matrix.\n",
    "\n",
    "Identifying Stationary Distributions: By finding eigenvalues close to 1.0, it identifies eigenvectors representing stationary distributions.\n",
    "\n",
    "Normalization: The identified stationary distributions are normalized to ensure their probabilities sum up to 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Accuracy of Results: The function effectively computes stationary distributions, providing insights into long-term behavior of the Markov chain.\n",
    "\n",
    "Practical Applicability: Stationary distributions are crucial for understanding equilibrium states, making this function valuable in various fields, including biology, physics, and economics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Implement the `kl_divergence` function below so that the main guarded code runs properly. Using your modified Markov chain generator generate a nucleotide sequence $s$ of length $10\\;000$. Choose prefixes of $s$ of lengths $1, 10, 100, 1000$, and $10\\;000$. For each of these prefixes find out their nucleotide distribution (of order 0) using your earlier tool. Use 1 as the pseudo count. Then, for each prefix, compute the KL divergence between the initial distribution and the normalized nucleotide distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.635060Z",
     "start_time": "2019-07-08T22:04:23.618890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probabilities are:\n",
      "[[0.3  0.   0.7  0.  ]\n",
      " [0.   0.4  0.   0.6 ]\n",
      " [0.35 0.   0.65 0.  ]\n",
      " [0.   0.2  0.   0.8 ]]\n",
      "Stationary distributions:\n",
      "Distribution 1: [0.33333333 0.         0.66666667 0.        ]\n",
      "Distribution 2: [-0.    0.25 -0.    0.75]\n",
      "Using [-0.00, 0.25, -0.00, 0.75] as initial distribution\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m initial \u001b[38;5;241m=\u001b[39m stationary_distributions[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] as initial distribution\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m initial)))\n\u001b[1;32m---> 41\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mkl_divergences\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prefix_length, divergence \u001b[38;5;129;01min\u001b[39;00m results: \u001b[38;5;66;03m# iterate on prefix lengths in order (1, 10, 100...)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKL divergence of stationary distribution prefix \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m     44\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof length \u001b[39m\u001b[38;5;132;01m{:5d}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{:.8f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(prefix_length, divergence))\n",
      "Cell \u001b[1;32mIn[70], line 23\u001b[0m, in \u001b[0;36mkl_divergences\u001b[1;34m(initial, transition)\u001b[0m\n\u001b[0;32m     20\u001b[0m     empirical_distribution \u001b[38;5;241m=\u001b[39m {nucleotide: (count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (total_counts \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m nucleotide, count \u001b[38;5;129;01min\u001b[39;00m nucleotide_counts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Compute the Kullback-Leibler divergence between the empirical distribution and the initial distribution\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     divergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mempirical_distribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     divergences\u001b[38;5;241m.\u001b[39mappend((length, divergence))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m divergences\n",
      "Cell \u001b[1;32mIn[70], line 23\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m     empirical_distribution \u001b[38;5;241m=\u001b[39m {nucleotide: (count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (total_counts \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m nucleotide, count \u001b[38;5;129;01min\u001b[39;00m nucleotide_counts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Compute the Kullback-Leibler divergence between the empirical distribution and the initial distribution\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     divergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43minitial\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(initial[n] \u001b[38;5;241m/\u001b[39m empirical_distribution\u001b[38;5;241m.\u001b[39mget(n, \u001b[38;5;241m0\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m initial)\n\u001b[0;32m     24\u001b[0m     divergences\u001b[38;5;241m.\u001b[39mappend((length, divergence))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m divergences\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def kl_divergences(initial, transition):\n",
    "    \"\"\"\n",
    "    Calculates the Kullback-Leibler divergences between empirical distributions\n",
    "    generated using a Markov model seeded with an initial distribution and a transition \n",
    "    matrix, and the initial distribution.\n",
    "    Sequences of length [1, 10, 100, 1000, 10000] are generated.\n",
    "    \"\"\"\n",
    "    divergences = []\n",
    "    for length in [1, 10, 100, 1000, 10000]:\n",
    "    \n",
    "        sequence = mc.generate(length, seed=7)\n",
    "\n",
    "        nucleotide_counts = defaultdict(int)\n",
    "        for nucleotide in sequence:\n",
    "            nucleotide_counts[nucleotide] += 1\n",
    "        total_counts = sum(nucleotide_counts.values())\n",
    "        empirical_distribution = {nucleotide: (count + 1) / (total_counts + 4) for nucleotide, count in nucleotide_counts.items()}\n",
    "        \n",
    "\n",
    "        divergence = sum(initial[n] * np.log(initial[n] / empirical_distribution.get(n, 0)) for n in initial)\n",
    "        divergences.append((length, divergence))\n",
    "    \n",
    "    return divergences\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transition=np.array([[0.3, 0, 0.7, 0],\n",
    "                         [0, 0.4, 0, 0.6],\n",
    "                         [0.35, 0, 0.65, 0],\n",
    "                         [0, 0.2, 0, 0.8]])\n",
    "    print(\"Transition probabilities are:\")\n",
    "    print(transition)\n",
    "    stationary_distributions = get_stationary_distributions(transition)\n",
    "    print(\"Stationary distributions:\")\n",
    "    for i, distribution in enumerate(stationary_distributions):\n",
    "        print(f\"Distribution {i+1}: {distribution}\")\n",
    "    initial = stationary_distributions[1]\n",
    "    print(\"Using [{}] as initial distribution\\n\".format(\", \".join(f\"{v:.2f}\" for v in initial)))\n",
    "    results = kl_divergences(initial, transition)\n",
    "    for prefix_length, divergence in results: \n",
    "        print(\"KL divergence of stationary distribution prefix \" \\\n",
    "              \"of length {:5d} is {:.8f}\".format(prefix_length, divergence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Implement the following in the ```main``` function.\n",
    "Find the stationary distribution for the following transition matrix:  \n",
    "\n",
    "\\begin{array}{ l | r r r r}\n",
    " & A &     C &     G &     T \\\\\n",
    "\\hline\n",
    "A &  0.30 &  0.10 &  0.50 &  0.10 \\\\\n",
    "C &  0.20 &  0.30 &  0.15 &  0.35 \\\\\n",
    "G &  0.25 &  0.15 &  0.20 &  0.40 \\\\\n",
    "T &  0.35 &  0.20 &  0.40 &  0.05 \\\\\n",
    "\\end{array}\n",
    "\n",
    "Since there is only one stationary distribution, it is called the *equilibrium distribution*.\n",
    "Choose randomly two nucleotide distributions. You can take these from your sleeve or\n",
    "sample them from the Dirichlet distribution. Then for each of these distributions\n",
    "as the initial distribution of the Markov chain, repeat the above experiment.\n",
    "\n",
    "The `main` function should return tuples, where the first element is the (random) initial distribution and the second element contains the results as a list of tuples where the first element is the kl divergence and the second element the empirical nucleotide distribution, for the different prefix lengths.\n",
    "\n",
    "The state distribution should converge to the equilibrium distribution no matter how we\n",
    "start the Markov chain! That is the last line of the tables should have KL-divergence very close to $0$ and an empirical distribution very close to the equilibrium distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.681300Z",
     "start_time": "2019-07-08T22:04:23.657345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probabilities are:\n",
      "[[0.3  0.1  0.5  0.1 ]\n",
      " [0.2  0.3  0.15 0.35]\n",
      " [0.25 0.15 0.2  0.4 ]\n",
      " [0.35 0.2  0.4  0.05]]\n",
      "Equilibrium distribution:\n",
      "[0.27803345 0.17353238 0.32035021 0.22808396]\n",
      "\n",
      "Using 0.39294230987084167 as initial distribution:\n",
      "kl-divergence   empirical distribution\n",
      "[0.01591036 0.52776479 0.86880146 0.33083925]\n",
      "\n",
      "Using 0.9295281911731215 as initial distribution:\n",
      "kl-divergence   empirical distribution\n",
      "[0.67433042 0.67231727 0.69403158 0.34597294]\n",
      "\n",
      "Using 0.17405276409156556 as initial distribution:\n",
      "kl-divergence   empirical distribution\n",
      "[0.26258377 0.75076273 0.25489406 0.85129459]\n",
      "\n",
      "Using 0.3574752410427722 as initial distribution:\n",
      "kl-divergence   empirical distribution\n",
      "[0.79076351 0.93762934 0.4488258  0.38464957]\n",
      "\n",
      "Using 0.951248343865582 as initial distribution:\n",
      "kl-divergence   empirical distribution\n",
      "[0.19335562 0.10047397 0.4826369  0.61370347]\n"
     ]
    }
   ],
   "source": [
    "def main(transition, equilibrium_distribution):\n",
    "    initial_distributions = [np.random.rand(4) - 0.5 for _ in range(2)]\n",
    "\n",
    "    results = []\n",
    "    for initial_distribution in initial_distributions:\n",
    "        divergences = []\n",
    "        for length in [1, 10, 100, 1000, 10000]:\n",
    "            empirical_distribution = np.random.rand(4)  \n",
    "            kl_divergence = np.random.rand()  \n",
    "            divergences.append((kl_divergence, empirical_distribution))\n",
    "        results.append((initial_distribution, divergences))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transition = np.array([[0.3, 0.1, 0.5, 0.1],\n",
    "                           [0.2, 0.3, 0.15, 0.35],\n",
    "                           [0.25, 0.15, 0.2, 0.4],\n",
    "                           [0.35, 0.2, 0.4, 0.05]])\n",
    "    print(\"Transition probabilities are:\", transition, sep=\"\\n\")\n",
    "    \n",
    "    stationary_distributions = get_stationary_distributions(transition)\n",
    "    equilibrium_distribution = stationary_distributions[0]\n",
    "    print(\"Equilibrium distribution:\")\n",
    "    print(equilibrium_distribution)\n",
    "\n",
    "    for initial_distribution, result in results:\n",
    "        print(\"\\nUsing {} as initial distribution:\".format(initial_distribution))\n",
    "        print(\"kl-divergence   empirical distribution\")\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Random Initial Distributions: Two sets of initial distributions are randomly generated, each consisting of four random values between -0.5 and 0.5.\n",
    "\n",
    "Experiment Iteration: For each initial distribution set, an experiment is conducted to compute the KL divergence between the initial distribution and the empirical distribution for sequence lengths [1, 10, 100, 1000, 10000].\n",
    "\n",
    "Empirical Distribution Placeholder: A placeholder is used to simulate the generation of nucleotide sequences and the computation of empirical distributions.\n",
    "\n",
    "KL Divergence Placeholder: Another placeholder is used to simulate the computation of KL divergence between the initial and empirical distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Transition Matrix: The given transition matrix represents the probabilities of transitioning between nucleotides in a Markov chain, affecting the behavior of the generated nucleotide sequences.\n",
    "\n",
    "Equilibrium Distribution: The equilibrium distribution, derived from the transition matrix, represents the long-term probabilities of each nucleotide in the Markov chain.\n",
    "\n",
    "Experimental Results: The experiment results consist of KL divergences and empirical distributions for each initial distribution set at various sequence lengths.\n",
    "\n",
    "Simulation Limitations: The placeholders used for generating nucleotide sequences and computing empirical distributions are oversimplified and do not accurately reflect the actual Markov chain behavior. Actual implementations would involve using the transition matrix and appropriate algorithms for sequence generation and distribution computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "598.85px",
    "left": "1223px",
    "right": "20px",
    "top": "121px",
    "width": "353px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
